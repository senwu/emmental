# Meta configuration
meta_config:
    seed: 0  # random seed for all numpy/torch/cuda operations in model and learning
    verbose: True # whether to print the log information
    log_path: # log directory

# Model configuration
model_config:
    model_path: # path to pretrained model
    device: 0 # -1 for cpu or gpu id (e.g., 0 for cuda:0)

# Learning configuration
learner_config:
    fp16: False # whether to use half precision
    n_epochs: 1 # total number of learning epochs
    train_split: train # the split for training, accepts str or list of strs
    valid_split: valid # the split for validation, accepts str or list of strs
    test_split: test # the split for testing, accepts str or list of strs
    ignore_index: -100 # the ignore index, uses for masking samples
    optimizer_config:
        optimizer: sgd # [sgd, adam]
        lr: 0.001 # Learing rate
        l2: 0.0 # l2 regularization
        grad_clip: 1.0 # gradient clipping
        sgd_config:
            momentum: 0.9
        adam_config:
            betas: !!python/tuple [0.9, 0.999]
    lr_scheduler_config:
        lr_scheduler: # [linear, exponential, reduce_on_plateau]
        warmup_steps: # warm up steps
        warmup_unit: batch # [epoch, batch]
        warmup_percentage: # warm up percentage
        min_lr: 0.0 # minimum learning rate
        linear_config:
            min_lr: 0.0
        exponential_config:
            gamma: 0.9
        plateau_config:
            factor: 0.5
            patience: 10
            threshold: 0.0001
    task_scheduler: round_robin # [sequential, round_robin]
    global_evaluation_metric_dict: # global evaluation metric dict

# Logging configuration
logging_config:
    counter_unit: batch # [epoch, batch]
    evaluation_freq: 2
    writer_config:
        writer: tensorboard # [json, tensorboard]
        verbose: True
    checkpointing: True
    checkpointer_config:
        checkpoint_path:
        checkpoint_freq: 1
        checkpoint_metric: # metric_name: mode, where mode in [min, max]
            # model/train/all/loss: min
        checkpoint_task_metrics: # task_metric_name: mode
        checkpoint_runway: 0 # checkpointing runway (no checkpointing before k unit)
        checkpoint_clear: True # whether to clear immedidate checkpointing

