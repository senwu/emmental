# Meta configuration
meta_config:
    seed: 1  # random seed for all numpy/torch/cuda operations in model and learning
    verbose: False # whether to print the log information
    log_path: tests # log directory

# Data configuration
data_config:
    min_data_len: 0 # min data length
    max_data_len: 0 # max data length (e.g., 0 for no max_len)

# Model configuration
model_config:
    model_path: # path to pretrained model
    device: 0 # -1 for cpu or gpu id (e.g., 0 for cuda:0)
    dataparallel: True # whether to use dataparallel or not

# Learning configuration
learner_config:
    fp16: False # whether to use half precision
    n_epochs: 1 # total number of learning epochs
    train_split: train # the split for training, accepts str or list of strs
    valid_split: valid # the split for validation, accepts str or list of strs
    test_split: test # the split for testing, accepts str or list of strs
    optimizer_config:
        optimizer: adam # [sgd, adam, adamax, bert_adam]
        lr: 0.001 # Learing rate
        l2: 0.0 # l2 regularization
        grad_clip: # gradient clipping
        asgd_config:
            lambd: 0.0001
            alpha: 0.75
            t0: 1000000.0
        adadelta_config:
            rho: 0.9
            eps: 0.000001
        adagrad_config:
            lr_decay: 0
            initial_accumulator_value: 0
            eps: 0.0000000001
        adam_config:
            betas: !!python/tuple [0.9, 0.999]
            eps: 0.00000001
            amsgrad: False
        adamw_config:
            betas: !!python/tuple [0.9, 0.999]
            eps: 0.00000001
            amsgrad: False
        adamax_config:
            betas: !!python/tuple [0.9, 0.999]
            eps: 0.00000001
        lbfgs_config:
            max_iter: 20
            max_eval:
            tolerance_grad: 0.0000001
            tolerance_change: 0.000000001
            history_size: 100
            line_search_fn:
        rms_prop_config:
            alpha: 0.99
            eps: 0.00000001
            momentum: 0
            centered: False
        r_prop_config:
            etas: !!python/tuple [0.5, 1.2]
            step_sizes: !!python/tuple [0.000001, 50]
        sgd_config:
            momentum: 0
            dampening: 0
            nesterov: False
        sparse_adam_config:
            betas: !!python/tuple [0.9, 0.999]
            eps: 0.00000001
        bert_adam_config:
            betas: !!python/tuple [0.9, 0.999]
            eps: 0.00000001
    lr_scheduler_config:
        lr_scheduler: # [linear, exponential, reduce_on_plateau, cosine_annealing]
        warmup_steps: # warm up steps
        warmup_unit: batch # [epoch, batch]
        warmup_percentage: # warm up percentage
        min_lr: 0.0 # minimum learning rate
        linear_config:
            min_lr: 0.0
        exponential_config:
            gamma: 0.9
        plateau_config:
            factor: 0.5
            patience: 10
            threshold: 0.0001
        step_config:
            step_size: 1
            gamma: 0.1
            last_epoch: -1
        multi_step_config:
            milestones:
                - 1000
            gamma: 0.1
            last_epoch: -1
        cosine_annealing_config:
            last_epoch: -1
    task_scheduler_config:
        task_scheduler: round_robin # [sequential, round_robin, mixed]
        sequential_scheduler_config:
            fillup: False
        round_robin_scheduler_config:
            fillup: False
        mixed_scheduler_config:
            fillup: False
    global_evaluation_metric_dict: # global evaluation metric dict

# Logging configuration
logging_config:
    counter_unit: epoch # [epoch, batch]
    evaluation_freq: 1
    writer_config:
        writer: tensorboard # [json, tensorboard]
        verbose: True
    checkpointing: False
    checkpointer_config:
        checkpoint_path:
        checkpoint_freq: 1
        checkpoint_metric: # metric_name: mode, where mode in [min, max]
            # model/train/all/loss: min
        checkpoint_task_metrics: # task_metric_name: mode
        checkpoint_runway: 0 # checkpointing runway (no checkpointing before k unit)
        clear_intermediate_checkpoints: True # whether to clear intermediate checkpoints
        clear_all_checkpoints: False # whether to clear all checkpoints
